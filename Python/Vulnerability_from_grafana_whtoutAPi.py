import http.client
import json
import time
import urllib.parse
from html.parser import HTMLParser

# Configurations
GRAFANA_HOST = "your-grafana-instance.com"
GRAFANA_LOGIN_PATH = "/login"  # Page de connexion Grafana
GRAFANA_VULNERABILITY_PAGE = "/dashboard/vulns"  # Page contenant les vulnérabilités
LDAP_USERNAME = "your-ldap-username"
LDAP_PASSWORD = "your-ldap-password"
FILTER_APPLICATION = "NomDeLApplication"
GOOGLE_SEARCH_URL = "www.google.com"

# Parser HTML pour extraire les vulnérabilités
class GrafanaParser(HTMLParser):
    def __init__(self):
        super().__init__()
        self.in_table = False
        self.current_column = None
        self.current_entry = {}
        self.vulnerabilities = []

    def handle_starttag(self, tag, attrs):
        if tag == "table":
            self.in_table = True
        elif self.in_table and tag == "tr":
            self.current_entry = {}
        elif self.in_table and tag == "td":
            self.current_column = "data"

    def handle_data(self, data):
        if self.in_table and self.current_column:
            self.current_entry[self.current_column] = data.strip()

    def handle_endtag(self, tag):
        if tag == "tr" and self.in_table and self.current_entry:
            self.vulnerabilities.append(self.current_entry)
        elif tag == "table":
            self.in_table = False

# Authentification sur Grafana via formulaire web
def authenticate():
    print("Authentification via SSO...")

    conn = http.client.HTTPSConnection(GRAFANA_HOST)
    headers = {"Content-Type": "application/x-www-form-urlencoded"}
    payload = urllib.parse.urlencode({"user": LDAP_USERNAME, "password": LDAP_PASSWORD})

    conn.request("POST", GRAFANA_LOGIN_PATH, body=payload, headers=headers)
    response = conn.getresponse()
    cookies = response.getheader("Set-Cookie")  # Récupération du cookie de session
    conn.close()

    if response.status == 200 and cookies:
        print("Authentification réussie. Cookie récupéré.")
        return cookies
    else:
        print(f"Échec de l'authentification: {response.status}")
        return None

# Récupération des vulnérabilités depuis la page web de Grafana
def get_vulnerabilities(cookies):
    print(f"Accès aux vulnérabilités pour l'application : {FILTER_APPLICATION}...")

    conn = http.client.HTTPSConnection(GRAFANA_HOST)
    headers = {
        "Cookie": cookies,
        "User-Agent": "Mozilla/5.0"
    }

    conn.request("GET", GRAFANA_VULNERABILITY_PAGE, headers=headers)
    response = conn.getresponse()
    data = response.read().decode()
    conn.close()

    if response.status == 200:
        parser = GrafanaParser()
        parser.feed(data)
        vulnerabilities = parser.vulnerabilities

        if vulnerabilities:
            print(f"{len(vulnerabilities)} vulnérabilité(s) trouvée(s).")
            return vulnerabilities
        else:
            print("Aucune vulnérabilité trouvée.")
            return []
    else:
        print(f"Erreur lors de l'accès aux vulnérabilités: {response.status}")
        return []

# Sauvegarde des vulnérabilités dans un fichier JSON
def save_to_json(vulnerabilities, filename="vulnerabilities.json"):
    with open(filename, "w", encoding="utf-8") as file:
        json.dump(vulnerabilities, file, indent=4, ensure_ascii=False)
    print(f"Vulnérabilités enregistrées dans {filename}.")

# Parser HTML pour extraire les liens Google
class GoogleParser(HTMLParser):
    def __init__(self):
        super().__init__()
        self.links = []

    def handle_starttag(self, tag, attrs):
        if tag == "a":
            for attr in attrs:
                if attr[0] == "href" and "url?q=" in attr[1]:
                    link = attr[1].split("url?q=")[1].split("&")[0]
                    if "http" in link:
                        self.links.append(link)

    def get_first_link(self):
        return self.links[0] if self.links else "Aucun lien trouvé"

# Recherche Google pour un plan de remédiation
def search_remediation(summary):
    print(f"Recherche du plan de remédiation pour : {summary}...")

    search_query = urllib.parse.quote(f"remédiation {summary}")
    conn = http.client.HTTPSConnection(GOOGLE_SEARCH_URL)
    headers = {"User-Agent": "Mozilla/5.0"}

    conn.request("GET", f"/search?q={search_query}", headers=headers)
    response = conn.getresponse()
    data = response.read().decode()
    conn.close()

    if response.status == 200:
        parser = GoogleParser()
        parser.feed(data)
        link = parser.get_first_link()
        print(f"Plan de remédiation trouvé: {link}")
        return link
    else:
        print("Échec de la recherche Google.")
        return None

# Enrichir le fichier JSON avec le plan de remédiation
def enrich_with_remediation(filename="vulnerabilities.json"):
    with open(filename, "r", encoding="utf-8") as file:
        vulnerabilities = json.load(file)

    for vulnerability in vulnerabilities:
        summary = vulnerability.get("data")  # Récupération du texte d'une vulnérabilité
        if summary:
            remediation_link = search_remediation(summary)
            if remediation_link:
                vulnerability["remediation"] = remediation_link
            time.sleep(2)  # Pause pour éviter les restrictions Google

    with open(filename, "w", encoding="utf-8") as file:
        json.dump(vulnerabilities, file, indent=4, ensure_ascii=False)

    print("Les vulnérabilités ont été enrichies avec des plans de remédiation.")

# Exécution du script
if __name__ == "__main__":
    cookies = authenticate()
    if cookies:
        vulnerabilities = get_vulnerabilities(cookies)
        if vulnerabilities:
            save_to_json(vulnerabilities)
            enrich_with_remediation()
